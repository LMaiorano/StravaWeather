In this document, we will present our evaluation of this Computer Science Project. To make sure that we covered all points, the evaluation is split in several parts. First, the team work is evaluated, after which the Development Plan is covered. Next, the implementation process is evaluated. Lastly, we reflect on the project as a whole.

**Team Work**

In Q1 of the Computer Science Minor we already worked together in the Software Engineering Methods project. During that project, we already worked together very well as a team.At the beginning of this project, we then decided to keep going the way we had before, since it worked out very well. With the end of this project, we all feel that we have become an even better team, since we all know much more about each other. Especially communication really improved.

During the previous project, team members did not always really know what the others were working. This project however, all team members really felt more engaged in the project and knew what the other team members were up to. This was partly due because of the weekly meetings, in which everybody updated the other team members about the work status and possible problems. 

**Development Plan**

We believe that the Development Plan really helped us to keep on track, although we were a bit unsure at first. This was mainly because we learned about the Agile methodology in the previous project, and a Development Plan felt like throwing Agile away and restarting with Waterfall again. However, in retrospective, we are glad that we had a Development Plan, because it helped guide us if we did not exactly know anymore what we had in mind at the beginning of the project.

For the requirements in the plan, all have been implemented. This deserves a little side note, since it is possible to set the date range as an input parameter, but for our project, this date range was always the full year 2019 to provide a more specified scope for our research question.

The list of constraints has grown a little since the beginning of the project, because of small onforseen influencing factors or scope limitations, like the fact that we had to specify a time frame for the KNMI website to scrape the weather data.

Of the MOSCOW list of features, all Must and Should Haves have been implemented. From the Could Have list, it is possible to have an interactive heat map, but it was decided to make use of matplotlib instead of Google Maps to make the validation easier. During the project, we stuck to the priorities set in the Development Plan, and also our Design Objectives and Strategy were maintained, as can also be read in the previous paragraph.

Because

**Implementation Process**

In comparison to the previous project, there was a clear Design Plan from the beginning, which helped a lot in dividing the work that needed to be done. For example, because we created a table of in- and output parameters of all modules, it was possible to start on different modules at the same time, without having data to test the modules, since everybody already knew what the format and structure of the input data would be. In combination with the very clear communication, this really helped the team to work on separate modules at the same time without losing track of what the other members were doing.

Even though we believe we started with a very thorough Development Plan, some changes were necessary. For example, the module that scrapes the data from the Strava website took much longer than expected, which resulted in a very short time left to finish the actual data analysis and results validation. Also, we did not anticipate that there would be such a huge amount of data available on Strava. Because of this, in combination with the severe limitations posed by the Strava API, we decided to limit our scope to Zuid-Holland instead.





**Overall**

